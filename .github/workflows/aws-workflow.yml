name: Deploy to Amazon EC2 - London

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:

env:
  AWS_REGION: eu-west-2                     # London region
  EC2_HOST: ${{ secrets.EC2_HOST }}         # Get from secrets instead of hardcoding
  EC2_INSTANCE_ID: ${{ secrets.EC2_INSTANCE_ID }} # Get from secrets instead of hardcoding
  NODE_VERSION: '18.x'                      # matches your engine requirements in package.json
  RDS_DB_INSTANCE: my-database              # RDS instance identifier
  RDS_DB_NAME: my_marketplace               # Database name
  RDS_DB_USER: marketplace_user             # Database user
  RDS_BACKUP_RETENTION: 7                   # Days to keep automated backups
  # RDS_DB_PASSWORD moved to secrets

permissions:
  contents: read
  id-token: write  # Required for requesting the JWT

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: '**/package-lock.json'

    # Install canvas dependencies before any npm operations
    - name: Install system dependencies for canvas
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libcairo2-dev libpango1.0-dev libjpeg-dev libgif-dev librsvg2-dev

    # Install global dependencies first
    - name: Install node-pre-gyp globally
      run: sudo npm install -g node-pre-gyp
      
    # Use --unsafe-perm for npm install to avoid permission issues
    - name: npm install
      run: npm install --no-bin-links --unsafe-perm

    # Fix permissions for all binaries in node_modules/.bin
    - name: Fix permissions for node binaries
      run: |
        find node_modules/.bin -type f -exec chmod +x {} \;
        echo "Fixed permissions for binaries in node_modules/.bin"

    # Rebuild canvas with --unsafe-perm and fallback message
    - name: Rebuild canvas module
      run: npm rebuild canvas --update-binary --unsafe-perm || echo "Canvas rebuild failed but continuing"

    # Update browserslist database to avoid warnings
    - name: Update browserslist DB
      run: npx update-browserslist-db@latest

    # Build step
    - name: Build
      run: npm run build --if-present
      env:
        NODE_ENV: test
        TEST_DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}
        JWT_SECRET: ${{ secrets.JWT_SECRET }}

    # Improved test step with better error handling and configuration
    - name: Run tests
      run: |
        echo "Verifying Jest binary..."
        if [ -f "node_modules/.bin/jest" ]; then
          chmod +x node_modules/.bin/jest
          echo "Jest binary found and permissions set"
        else
          echo "Jest binary not found in expected location, installing Jest CLI..."
          npm install --no-save jest jest-cli
        fi
        
        # Show Jest version for debugging
        echo "Jest version:"
        npx jest --version
        
        # Verify Jest config exists
        if [ -f "jest.config.js" ]; then
          echo "Jest config found at jest.config.js"
        elif grep -q '"jest":' package.json; then
          echo "Jest config found in package.json"
        else
          echo "Warning: No Jest configuration found, using defaults"
        fi
        
        # Run tests with more debugging options
        echo "Running tests..."
        npx jest --passWithNoTests --runInBand --ci --verbose
      env:
        NODE_ENV: test
        TEST_DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}
        JWT_SECRET: ${{ secrets.JWT_SECRET }}
        # Set NODE_OPTIONS to increase memory limit if needed
        NODE_OPTIONS: "--max_old_space_size=4096"
        
    # Create deployment package
    - name: Create deployment package
      run: |
        # Create a directory for production files
        mkdir -p dist
        
        # Copy necessary files for deployment (excluding development files)
        rsync -av --exclude='.git' --exclude='.github' --exclude='node_modules' --exclude='tests' \
          --exclude='*.md' --exclude='*.env' --exclude='*.log' . dist/
          # Create package.json without dev dependencies for production
        jq 'del(.devDependencies)' package.json > dist/package.json
        
        # Create deployment archive
        cd dist && zip -r ../deployment.zip . 
        cd .. 
        echo "Created deployment package: deployment.zip"

    - name: Upload artifact for deployment job
      uses: actions/upload-artifact@v4
      with:
        name: node-app
        path: deployment.zip
        retention-days: 7

  deploy:
    name: Deploy
    needs: build
    runs-on: ubuntu-latest
    # Fix for "Skip setting environment url" error
    environment:
      name: 'Production'
      url: http://18.168.205.177:3000  # Use hardcoded URL instead of env reference

    steps:
    - name: Download artifact from build job
      uses: actions/download-artifact@v4
      with:
        name: node-app
        
    # Configure AWS credentials with explicit error handling
    - name: Configure AWS credentials
      id: aws-credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        mask-aws-account-id: true

    # Enhanced AWS CLI verification with better error messages
    - name: Verify AWS CLI
      run: |
        # Print AWS CLI version for debugging
        aws --version
        
        echo "Testing AWS credentials with basic command..."
        if ! aws sts get-caller-identity --query "Account" --output text > /dev/null 2>&1; then
          echo "ERROR: AWS authentication failed. Possible causes:"
          echo "1. AWS_ACCESS_KEY_ID or AWS_SECRET_ACCESS_KEY may be incorrect"
          echo "2. The IAM user/role may not have sufficient permissions"
          echo "3. AWS_REGION (${{ env.AWS_REGION }}) may be incorrect"
          exit 1
        fi
        
        echo "AWS authentication successful!"
        
        # Print last few characters of the account ID for verification without exposing full ID
        ACCOUNT_ID=$(aws sts get-caller-identity --query "Account" --output text)
        echo "Connected to AWS account ending in: ...${ACCOUNT_ID: -4}"
        
    # Simplified EC2 instance status check with better error handling
    - name: Check EC2 instance status
      run: |
        echo "Checking EC2 instance ID format..."
        EC2_ID="${{ secrets.EC2_INSTANCE_ID }}"
        
        # Echo part of the ID for debugging without exposing full ID
        echo "Instance ID ends with: ...${EC2_ID: -4}"
        
        # Validate format first
        if [[ ! "$EC2_ID" =~ ^i-[a-z0-9]+$ ]]; then
          echo "ERROR: Malformed EC2 instance ID. Should start with 'i-' followed by alphanumeric characters."
          exit 1
        fi
        
        echo "Verifying EC2 instance exists..."
        if ! aws ec2 describe-instance-status --instance-ids "$EC2_ID" > /dev/null 2>&1; then
          echo "ERROR: EC2 instance validation failed. Possible causes:"
          echo "1. Instance ID may not exist"
          echo "2. Instance may be in a different region than ${{ env.AWS_REGION }}"
          echo "3. IAM credentials may not have ec2:DescribeInstances permission"
          
          # Try listing available instances to help debugging
          echo "Available instances in region ${{ env.AWS_REGION }}:"
          aws ec2 describe-instances --query "Reservations[*].Instances[*].[InstanceId]" --output text
          
          exit 1
        fi

        echo "EC2 instance validation successful!"
        
        # Get and check instance state more carefully
        echo "Getting instance state..."
        INSTANCE_STATE=$(aws ec2 describe-instances --instance-ids "$EC2_ID" \
          --query "Reservations[0].Instances[0].State.Name" --output text || echo "error")
          
        if [ "$INSTANCE_STATE" = "error" ]; then
          echo "ERROR: Failed to get instance state"
          exit 1
        elif [ "$INSTANCE_STATE" != "running" ]; then
          echo "Instance is not running. Current state: $INSTANCE_STATE"
          echo "Starting the instance..."
          aws ec2 start-instances --instance-ids "$EC2_ID" > /dev/null
          
          echo "Waiting for instance to be running..."
          aws ec2 wait instance-running --instance-ids "$EC2_ID"
          
          echo "Instance is now running. Waiting 30 seconds for services to start..."
          sleep 30
        else
          echo "Instance is already running."
        fi
        
        # Get EC2 instance security group information
        echo "Checking EC2 security group configuration..."
        EC2_SG=$(aws ec2 describe-instances \
          --instance-ids "${{ secrets.EC2_INSTANCE_ID }}" \
          --query "Reservations[0].Instances[0].SecurityGroups[0].GroupId" \
          --output text)
          
        echo "EC2 instance security group: $EC2_SG"
        
        # Get GitHub Actions runner IP address
        GITHUB_RUNNER_IP=$(curl -s https://api.ipify.org)
        echo "GitHub Actions runner IP: $GITHUB_RUNNER_IP"
        
        # Check if security group has an SSH rule for the GitHub runner IP
        echo "Checking if security group allows SSH from GitHub Actions IP..."
        if ! aws ec2 describe-security-groups \
          --group-ids "$EC2_SG" \
          --filters "Name=ip-permission.protocol,Values=tcp" \
                   "Name=ip-permission.from-port,Values=22" \
                   "Name=ip-permission.to-port,Values=22" \
                   "Name=ip-permission.cidr,Values=$GITHUB_RUNNER_IP/32" \
          --query "SecurityGroups[*]" \
          --output text | grep -q .; then
          
          echo "Adding SSH rule for GitHub Actions runner IP to security group..."
          aws ec2 authorize-security-group-ingress \
            --group-id "$EC2_SG" \
            --protocol tcp \
            --port 22 \
            --cidr "$GITHUB_RUNNER_IP/32"
            
          echo "Added SSH rule for GitHub Actions runner IP: $GITHUB_RUNNER_IP"
          echo "Waiting 10 seconds for rule to propagate..."
          sleep 10
        else
          echo "Security group already allows SSH from GitHub Actions IP"
        fi
        
        # Check if general SSH access is allowed
        echo "Checking if general SSH access is allowed..."
        if ! aws ec2 describe-security-groups \
          --group-ids "$EC2_SG" \
          --filters "Name=ip-permission.protocol,Values=tcp" \
                   "Name=ip-permission.from-port,Values=22" \
                   "Name=ip-permission.to-port,Values=22" \
                   "Name=ip-permission.cidr,Values=0.0.0.0/0" \
          --query "SecurityGroups[*]" \
          --output text | grep -q .; then
          
          echo "Adding general SSH access rule to security group..."
          aws ec2 authorize-security-group-ingress \
            --group-id "$EC2_SG" \
            --protocol tcp \
            --port 22 \
            --cidr "0.0.0.0/0"
            
          echo "Added general SSH access rule"
          echo "Waiting 10 seconds for rule to propagate..."
          sleep 10
        else
          echo "Security group already allows SSH from any IP"
        fi

    # Improved SSH key setup with better error handling and key verification
    - name: Setup SSH key
      run: |
        mkdir -p ~/.ssh
        
        # Check if SSH key secret exists and has content
        if [ -z "${{ secrets.EC2_SSH_KEY }}" ]; then
          echo "ERROR: EC2_SSH_KEY secret is empty or not set"
          echo "Please add your private SSH key (content of .ssh\myec2keypair2.pem) as a GitHub secret named EC2_SSH_KEY"
          exit 1
        fi
        
        # Write the key with proper formatting
        echo "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/ec2_key.pem
        
        # Verify the key was written properly
        if [ ! -s ~/.ssh/ec2_key.pem ]; then
          echo "ERROR: SSH key file is empty after writing"
          exit 1
        fi
        
        # Set proper permissions
        chmod 600 ~/.ssh/ec2_key.pem
        
        # Verify key format (should start with -----BEGIN)
        if ! grep -q "BEGIN" ~/.ssh/ec2_key.pem; then
          echo "ERROR: SSH key appears to be invalid - missing BEGIN line"
          echo "Make sure you added the ENTIRE private key including BEGIN and END lines"
          exit 1
        fi
        
        echo "SSH key appears to be valid"
        
        # Verify EC2 instance is reachable first
        echo "Verifying EC2 instance is reachable at ${{ env.EC2_HOST }}..."
        if ! timeout 5 bash -c "</dev/tcp/${{ env.EC2_HOST }}/22"; then
          echo "ERROR: Cannot connect to EC2 host on port 22 (SSH)"
          echo "Check if:"
          echo "1. The EC2 instance is running"
          echo "2. The EC2_HOST value is correct: ${{ env.EC2_HOST }}"
          echo "3. The security group allows inbound SSH (port 22)"
          
          # Additional check: Try to ping the host
          echo "Trying to ping the host..."
          ping -c 3 ${{ env.EC2_HOST }} || echo "Ping failed, host may be unreachable"
          
          # Check if this is running in GitHub Actions and if so, show their outgoing IP
          echo "GitHub Actions runner outgoing IP address:"
          curl -s https://api.ipify.org || echo "Could not determine IP"
          
          exit 1
        fi
        
        echo "EC2 instance is reachable on port 22"
        
        # Add host to known_hosts safely with retries
        echo "Adding host key to known_hosts..."
        RETRY=0
        MAX_RETRIES=3
        
        while [ $RETRY -lt $MAX_RETRIES ]; do
          if ssh-keyscan -H ${{ env.EC2_HOST }} >> ~/.ssh/known_hosts 2>/dev/null; then
            echo "Host key added successfully"
            break
          else
            RETRY=$((RETRY+1))
            if [ $RETRY -eq $MAX_RETRIES ]; then
              echo "ERROR: Failed to scan host key for ${{ env.EC2_HOST }} after $MAX_RETRIES attempts"
              echo "Make sure the EC2_HOST value is correct and the instance is running"
              exit 1
            fi
            echo "Retry $RETRY/$MAX_RETRIES: Waiting 5 seconds before next attempt..."
            sleep 5
          fi
        done
        
        # Test SSH connection with verbose output for debugging
        echo "Testing SSH connection..."
        ssh -v -i ~/.ssh/ec2_key.pem -o BatchMode=yes -o ConnectTimeout=10 ec2-user@${{ env.EC2_HOST }} "echo 'SSH connection successful'" || {
          echo "ERROR: SSH connection failed"
          echo "Possible issues:"
          echo "1. EC2 instance may not be running"
          echo "2. Security group may not allow SSH connections from GitHub Actions IP"
          echo "3. SSH key in GitHub secrets may not match the key authorized on EC2"
          echo "4. EC2_HOST may be incorrect or the key not authorized for ec2-user"
          
          # Try another common EC2 username if ec2-user fails
          echo "Trying alternative username (ubuntu)..."
          ssh -v -i ~/.ssh/ec2_key.pem -o BatchMode=yes -o ConnectTimeout=5 ubuntu@${{ env.EC2_HOST }} "echo 'SSH connection successful'" && \
          echo "Connection successful with ubuntu user - please update your workflow to use ubuntu instead of ec2-user" || \
          echo "Connection also failed with ubuntu user"
          
          exit 1
        }

    # First deployment step - Ensure SSH key is authorized on EC2 instance
    - name: Authorize SSH key (if needed)
      run: |
        # This step tries to use AWS Systems Manager (SSM) to authorize the key
        # Make sure your IAM role has permissions for SSM
        echo "Attempting to authorize SSH key on EC2 instance..."
        
        # Get public key from the private key
        ssh-keygen -y -f ~/.ssh/ec2_key.pem > ~/.ssh/ec2_key.pub
        PUBLIC_KEY=$(cat ~/.ssh/ec2_key.pub)
        
        # Use SSM to add the key to authorized_keys
        aws ssm send-command \
          --instance-ids ${{ secrets.EC2_INSTANCE_ID }} \
          --document-name "AWS-RunShellScript" \
          --comment "Add SSH key to authorized_keys" \
          --parameters commands="[ \"mkdir -p /home/ec2-user/.ssh\", \"echo '$PUBLIC_KEY' >> /home/ec2-user/.ssh/authorized_keys\", \"chmod 700 /home/ec2-user/.ssh\", \"chmod 600 /home/ec2-user/.ssh/authorized_keys\", \"chown -R ec2-user:ec2-user /home/ec2-user/.ssh\" ]" \
          --output text || echo "SSM command failed - may need to authorize key manually"
        
        # Wait a bit for the command to complete
        sleep 10
        
        # Test SSH connection again
        echo "Testing SSH connection after key authorization..."
        ssh -i ~/.ssh/ec2_key.pem -o BatchMode=yes -o ConnectTimeout=5 ec2-user@${{ env.EC2_HOST }} "echo 'SSH connection successful'" || {
          echo "SSH still failing. You may need to manually authorize the key on the EC2 instance."
          echo "Instructions:"
          echo "1. Log into your EC2 instance using the AWS console"
          echo "2. Run the following commands:"
          echo "   mkdir -p ~/.ssh"
          echo "   # Add your public key to authorized_keys" 
          echo "   # (Public key from your local .ssh\\myec2keypair2.pem file)"
          echo "   chmod 700 ~/.ssh"
          echo "   chmod 600 ~/.ssh/authorized_keys"
          
          # Continue workflow despite error - future steps will fail if SSH still doesn't work
          echo "Continuing workflow, but deployment may fail if SSH authorization is not fixed"
        }

    # Check if RDS instance exists, create if it doesn't
    - name: Check RDS instance
      run: |
        echo "Checking if RDS instance exists..."
        
        # Check if the RDS instance already exists
        if aws rds describe-db-instances --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} > /dev/null 2>&1; then
          echo "RDS instance ${{ env.RDS_DB_INSTANCE }} already exists"
          
          # Get RDS endpoint
          RDS_ENDPOINT=$(aws rds describe-db-instances \
            --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} \
            --query 'DBInstances[0].Endpoint.Address' --output text)
          
          echo "RDS endpoint: $RDS_ENDPOINT"
          echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
          
          # Modify the RDS instance to be publicly accessible if it's not already
          PUBLICLY_ACCESSIBLE=$(aws rds describe-db-instances \
            --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} \
            --query 'DBInstances[0].PubliclyAccessible' --output text)
            
          if [ "$PUBLICLY_ACCESSIBLE" = "false" ]; then
            echo "Modifying RDS instance to be publicly accessible..."
            aws rds modify-db-instance \
              --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} \
              --publicly-accessible \
              --apply-immediately
              
            echo "Waiting for RDS instance to be updated..."
            aws rds wait db-instance-available \
              --db-instance-identifier ${{ env.RDS_DB_INSTANCE }}
          fi
          
          # Get the security group ID for the RDS instance
          RDS_SG_ID=$(aws rds describe-db-instances \
            --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} \
            --query 'DBInstances[0].VpcSecurityGroups[0].VpcSecurityGroupId' \
            --output text)
            
          # Add inbound rule to allow connections from anywhere
          echo "Updating security group to allow public access..."
          aws ec2 authorize-security-group-ingress \
            --group-id $RDS_SG_ID \
            --protocol tcp \
            --port 5432 \
            --cidr 0.0.0.0/0 \
            || echo "Rule for public access already exists"
            
        else
          echo "Creating new RDS PostgreSQL instance..."
          
          # Create a security group for the RDS instance
          RDS_SG_NAME="marketplace-db-sg"
          
          # Check if security group already exists
          SG_CHECK=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=$RDS_SG_NAME" --query 'SecurityGroups[0].GroupId' --output text)
          
          if [[ "$SG_CHECK" == "None" || -z "$SG_CHECK" ]]; then
            echo "Creating new security group $RDS_SG_NAME"
            RDS_SG_ID=$(aws ec2 create-security-group \
              --group-name $RDS_SG_NAME \
              --description "Security group for Marketplace RDS" \
              --output text --query 'GroupId')
          else
            echo "Security group $RDS_SG_NAME already exists, using existing one"
            RDS_SG_ID=$SG_CHECK
          fi
          
          echo "Using security group: $RDS_SG_ID"
          
          # Allow access from the EC2 instance
          EC2_PUBLIC_IP=$(aws ec2 describe-instances \
            --instance-ids ${{ env.EC2_INSTANCE_ID }} \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text)
          
          aws ec2 authorize-security-group-ingress \
            --group-id $RDS_SG_ID \
            --protocol tcp \
            --port 5432 \
            --cidr $EC2_PUBLIC_IP/32 \
            || echo "Rule for EC2 instance already exists"
          
          # Allow public access for pgAdmin connections
          aws ec2 authorize-security-group-ingress \
            --group-id $RDS_SG_ID \
            --protocol tcp \
            --port 5432 \
            --cidr 0.0.0.0/0 \
            || echo "Rule for public access already exists"
          
          # Create the RDS instance with public access
          aws rds create-db-instance \
            --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} \
            --db-instance-class db.t3.micro \
            --engine postgres \
            --engine-version 15 \
            --allocated-storage 20 \
            --master-username ${{ env.RDS_DB_USER }} \
            --master-user-password "${{ secrets.RDS_DB_PASSWORD }}" \
            --vpc-security-group-ids $RDS_SG_ID \
            --backup-retention-period ${{ env.RDS_BACKUP_RETENTION }} \
            --publicly-accessible \
            --db-name ${{ env.RDS_DB_NAME }}
          
          echo "Waiting for RDS instance to be available..."
          aws rds wait db-instance-available \
            --db-instance-identifier ${{ env.RDS_DB_INSTANCE }}
          
          # Get RDS endpoint
          RDS_ENDPOINT=$(aws rds describe-db-instances \
            --db-instance-identifier ${{ env.RDS_DB_INSTANCE }} \
            --query 'DBInstances[0].Endpoint.Address' --output text)
          
          echo "RDS instance created with endpoint: $RDS_ENDPOINT"
          echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
        fi
        
        # Create database URL for the application - ensure RDS_ENDPOINT is set
        if [ -z "$RDS_ENDPOINT" ]; then
          echo "Error: RDS_ENDPOINT is not set, cannot create database URL"
          exit 1
        fi
        
        DB_URL="postgresql://${{ env.RDS_DB_USER }}:${{ secrets.RDS_DB_PASSWORD }}@$RDS_ENDPOINT:5432/${{ env.RDS_DB_NAME }}"
        echo "DB_URL=$DB_URL" >> $GITHUB_ENV
        
        # Print connection info for pgAdmin without exposing password
        echo "---------------------------------------------"
        echo "RDS Connection Info for pgAdmin:"
        echo "Hostname: $RDS_ENDPOINT"
        echo "Port: 5432" 
        echo "Database: ${{ env.RDS_DB_NAME }}"
        echo "Username: ${{ env.RDS_DB_USER }}"
        echo "Password: [REDACTED - Stored in GitHub Secrets]" 
        echo "---------------------------------------------"

    # Deploy to EC2 with improved error handling and PM2 fixes
    - name: Deploy to EC2
      run: |
        set -x  # Print commands before execution
        echo "Deploying application to EC2 instance..."
        
        # Verify essential variables
        if [ -z "${{ env.EC2_HOST }}" ]; then
          echo "Error: EC2_HOST is not set. Check your GitHub secrets."
          exit 1
        fi
        
        # Verify that DB_URL is set
        if [ -z "$DB_URL" ]; then
          echo "Error: Database URL is not set"
          exit 1
        fi
        
        # Create environment file locally
        echo "NODE_ENV=production" > env_file
        echo "DATABASE_URL=$DB_URL" >> env_file
        echo "DATABASE_SSL=true" >> env_file
        echo "JWT_SECRET=${{ secrets.JWT_SECRET }}" >> env_file
        echo "PORT=3000" >> env_file
        
        # Configure SSH to be more resilient
        mkdir -p ~/.ssh/config.d
        cat > ~/.ssh/config <<EOF
        Host ${{ env.EC2_HOST }}
          ServerAliveInterval 60
          ServerAliveCountMax 10
          TCPKeepAlive yes
          ConnectTimeout 180
          StrictHostKeyChecking no
        EOF
        
        chmod 600 ~/.ssh/config
        
        # Transfer files to EC2 with retry
        echo "Transferring deployment files to EC2..."
        for i in {1..3}; do
          if scp -i ~/.ssh/ec2_key.pem deployment.zip ec2-user@${{ env.EC2_HOST }}:~/deployment.zip; then
            echo "Deployment package transferred successfully"
            break
          else
            echo "Transfer failed, attempt $i of 3"
            if [ $i -eq 3 ]; then
              echo "Failed to transfer deployment package after 3 attempts. Exiting."
              exit 1
            fi
            sleep 5
          fi
        done
        
        # Transfer env file with retry
        for i in {1..3}; do
          if scp -i ~/.ssh/ec2_key.pem env_file ec2-user@${{ env.EC2_HOST }}:~/env_file; then
            echo "Environment file transferred successfully"
            break
          else
            echo "Transfer failed, attempt $i of 3"
            if [ $i -eq 3]; then
              echo "Failed to transfer environment file after 3 attempts. Exiting."
              exit 1
            fi
            sleep 5
          fi
        done
        
        # Execute deployment commands on EC2 in smaller batches to prevent disconnection
        echo "Connecting to EC2 and creating app directory..."
        ssh -i ~/.ssh/ec2_key.pem ec2-user@${{ env.EC2_HOST }} "mkdir -p ~/app"
        
        echo "Backing up current deployment if it exists..."
        ssh -i ~/.ssh/ec2_key.pem ec2-user@${{ env.EC2_HOST }} "
          if [ -d ~/app/current ]; then
            timestamp=\$(date +%Y%m%d%H%M%S)
            mkdir -p ~/app/backup
            mv ~/app/current ~/app/backup/backup_\$timestamp
          fi
        "
        
        echo "Preparing new deployment directory..."
        ssh -i ~/.ssh/ec2_key.pem ec2-user@${{ env.EC2_HOST }} "
          mkdir -p ~/app/current
          unzip -o ~/deployment.zip -d ~/app/current
          mv ~/env_file ~/app/current/.env
          cd ~/app/current
          echo 'Deployment files extracted successfully'
        "
        
        echo "Installing dependencies..."
        ssh -i ~/.ssh/ec2_key.pem ec2-user@${{ env.EC2_HOST }} "
          cd ~/app/current
          # Use updated npm flag syntax
          npm install --omit=dev
          echo 'Dependencies installed successfully'
        "
        
        echo "Setting up and starting PM2..."
        ssh -i ~/.ssh/ec2_key.pem ec2-user@${{ env.EC2_HOST }} "
          cd ~/app/current
          
          # Setup and start PM2 with proper paths and error handling
          echo 'Setting up PM2...'
          which pm2 || sudo npm install -g pm2
          
          # Make sure to use the full path to PM2
          export PATH=\$PATH:/usr/local/bin:/home/ec2-user/.npm-global/bin
          PM2_PATH=\$(which pm2)
          
          if [ -z \"\$PM2_PATH\" ]; then
            echo 'PM2 not found in PATH, trying specific locations'
            PM2_PATH=/usr/local/bin/pm2
            if [ ! -f \$PM2_PATH ]; then
              PM2_PATH=/home/ec2-user/.npm-global/bin/pm2
              if [ ! -f \$PM2_PATH]; then
                PM2_PATH=\$(find /usr -name pm2 2>/dev/null | head -1)
                if [ -z \"\$PM2_PATH\" ]; then
                  echo 'ERROR: PM2 not found - installing globally'
                  sudo npm install -g pm2
                  PM2_PATH=\$(which pm2)
                fi
              fi
            fi
          fi
          
          echo \"Using PM2 at: \$PM2_PATH\"
          
          # Use PM2 with full path to avoid any issues
          \$PM2_PATH stop marketplace 2>/dev/null || true
          \$PM2_PATH start server.js --name marketplace
          \$PM2_PATH save
          \$PM2_PATH resurrect || true
          
          # Cleanup
          rm ~/deployment.zip
          echo 'Deployment completed successfully!'
        "

    # Verify deployment
    - name: Verify deployment
      run: |
        echo "Verifying deployment..."
        sleep 5
        
        # Use curl to check if the application is responding
        response_code=$(curl -s -o /dev/null -w "%{http_code}" http://${{ env.EC2_HOST }}:3000 || echo "failed")
        
        if [[ $response_code == "200" ]]; then
          echo "✅ Application successfully deployed and responding with status code 200"
          echo "Application URL: http://${{ env.EC2_HOST }}:3000"
        else
          echo "⚠️ Application check returned status code: $response_code"
          echo "It may take a few moments for the application to fully start"
          echo "Application URL: http://${{ env.EC2_HOST }}:3000"
          
          # Don't fail the workflow, as the app might just need more time to start
          echo "Deployment completed, but application verification needs manual check"
        fi